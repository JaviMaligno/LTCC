\documentclass{article}
\usepackage{amsmath,accents}%
\usepackage{amsfonts}%
\usepackage{amssymb}%
\usepackage{comment}
\usepackage{graphicx}
\usepackage{mathrsfs}
\usepackage[utf8]{inputenc}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{mathrsfs}
\usepackage{setspace}  
\usepackage{amsthm}
\usepackage{nccmath}
\usepackage[UKenglish]{babel}
\usepackage{multirow}
\usepackage{enumerate}
\usepackage{listings}
\usepackage{pgf,tikz}
\usetikzlibrary{arrows}

\theoremstyle{plain}

\renewcommand{\baselinestretch}{1,4}
\setlength{\oddsidemargin}{0.5in}
\setlength{\evensidemargin}{0.5in}
\setlength{\textwidth}{5.4in}
\setlength{\topmargin}{-0.25in}
\setlength{\headheight}{0.5in}
\setlength{\headsep}{0.6in}
\setlength{\textheight}{8in}
\setlength{\footskip}{0.75in}

\newtheorem{theorem}{Teorema}[section]
\newtheorem{acknowledgement}{Acknowledgement}
\newtheorem{algorithm}{Algorithm}
\newtheorem{axiom}{Axiom}
\newtheorem{case}{Case}
\newtheorem{claim}{Claim}
\newtheorem{propi}[theorem]{Propiedades}
\newtheorem{condition}{Condition}
\newtheorem{conjecture}{Conjecture}
\newtheorem{coro}[theorem]{Corolario}
\newtheorem{criterion}{Criterion}
\newtheorem{defi}[theorem]{Definición}
\newtheorem{example}[theorem]{Ejemplo}

\theoremstyle{definition}
\newtheorem{exerciseaux}{Exercise}
\newtheorem{lemma}[theorem]{Lema}
\newtheorem{nota}[theorem]{Nota}
%\newtheorem{sol}{Solución}
%\newtheorem*{sol*}{Solution}
\newtheorem{prop}[theorem]{Proposición}
\newtheorem{remark}{Remark}
\newenvironment{exercise}[1]
  {\renewcommand\theexerciseaux{#1}\exerciseaux\label{ejer:#1}}
  {\endexerciseaux}
\newcounter{exercounter}[section]
\setcounter{exercounter}{\value{exerciseaux}}
\newenvironment{sol}{\begin{trivlist}
 \item[\hskip \labelsep {\textit{Solution}.}\hskip \labelsep]}{\end{trivlist}}

\newtheorem{dem}[theorem]{Demostración}

\newtheorem{summary}{Summary}

\providecommand{\abs}[1]{\lvert#1\rvert}
\providecommand{\norm}[1]{\lVert#1\rVert}
\providecommand{\ninf}[1]{\norm{#1}_\infty}
\providecommand{\numn}[1]{\norm{#1}_1}
\providecommand{\gabs}[1]{\left|{#1}\right|}
\newcommand{\bor}[1]{\mathcal{B}(#1)}
\newcommand{\parcial}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\secondparcial}[2]{\frac{\partial^2 #1}{\partial {#2}^2}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\X}{\chi}
\providecommand{\Zn}[1]{\Z / \Z #1}
\newcommand{\resi}{\varepsilon_L}
\newcommand{\cee}{\mathbb{C}}
\providecommand{\conv}[1]{\overset{#1}{\longrightarrow}}
\providecommand{\gene}[1]{\langle{#1}\rangle}
\providecommand{\convcs}{\xrightarrow{CS}}
% xrightarrow{d}[d]
%\setcounter{exercise}{0}
\newcommand{\cicl}{\mathcal{C}}

\newenvironment{ejercicio}[2][Estado]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries Ejercicio}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
%--------------------------------------------------------
\begin{document}

\title{Applied Bayesian Methods - Exam }
\author{Javier Aguilar Martín}
\date{\today}
\maketitle
\begin{exercise}{A1}
\end{exercise}
\begin{sol}
Let us call $T$ the event of testing positive and $B$ the event of being pregnant. The information that we have is $P(T\mid B)=\theta$, $P(T\mid \neg B)=\lambda$ and $P(B)=q$.

\begin{enumerate}[(a)]
\item By Bayes' theorem we have
\[
P(B\mid \neg T) = \frac{P(\neg T\mid B)P(B)}{P(\neg T)} = \frac{(1-P(T\mid B))P(B)}{1-P(T)}=\frac{(1-\theta)q}{1-\theta q-\lambda (1-q)}
\]
\item Let $T_1$ the event of testing negative in the first test and $T_2$ the event of testing negative in the second test. We may assumet that $P(T_i\mid B)=(1-\theta)$, $P(T_i\mid \neg B)=(1-\lambda)$ for $i=1,2$. Then by conditional independence and the calculation from the previous part we have
\begin{align*}
P(T_2\mid T_1) &= P(T_2,B\mid T_1)+P(T_2,\neg B\mid T_1)\\
 &= P(T_2\mid B)P(B\mid T_2)+P(T_2\mid \neg B)P(\neg B\mid T_1)\\
 &= \frac{(1-\theta)^2 q}{1-\theta q - \lambda(1-q)}+(1-\lambda)\left(1-\frac{(1-\theta) q}{1-\theta q - \lambda(1-q)}\right)
\end{align*}
If we take $A=\frac{\theta(1-q)-\lambda(1-q)}{1-\lambda}$ it is clear that the denominators can be writen as $(1-\theta)+A(1-\lambda)$. Therefore, if we write the result as a single fraction we get
\begin{align*}
\frac{(1-\theta)^2 q + (1-\lambda)((1-\theta)+A(1-\lambda))-(1-\lambda)(1-\theta)q}{(1-\theta)+A(1-\lambda)}
\end{align*}
which can be rewrriten as
\[
\frac{A(1-\lambda)^2+(1-\theta)^2}{(1-\theta)+A(1-\lambda)}
\]
\item Let $n=20$ and $Y$ the random variable accounting for the number of tests. We have that $P(Y=y\mid\theta)=\binom{n}{y}\theta^y(1-\theta)^{n-y}$ where $\theta\sim\mathrm{Beta}(\alpha,\beta)$. The posterior distribution can then be computed using Bayes' theorem as
\begin{align*}
P(\theta\mid Y=y)&=\frac{P(Y=y\mid\theta)P(\theta)}{P(Y=y)}\\
& =\binom{n}{y}\frac{\theta^y(1-\theta)^{n-y}}{P(Y=y)}\frac{\Gamma(\alpha+\beta)\theta^{\alpha-1}(1-\theta)^{\beta-1}}{\Gamma(\alpha)\Gamma(\beta)}\\
&= \binom{n}{y}\frac{\Gamma(\alpha+\beta)}{P(Y=y)\Gamma(\alpha)\Gamma(\beta)}\theta^{\alpha+y-1}(1-\theta)^{\beta+n-y-1}
\end{align*}
This shows that $\theta\mid y\sim\mathrm{Beta}(\alpha+y, \beta+n-y)$. One can compute the value of the normalizing constant $P(Y=y)$ by integrating with respect to $\theta$ on both sides. Using that $\Gamma(m)=(m-1)!$ for $m\in\mathbb{N}$ this gives
\[
P(Y=y)= \frac{\Gamma(n+1)\Gamma(\alpha+\beta)\Gamma(\alpha+y)\Gamma(\beta +n-y)}{\Gamma(y+1)\Gamma(n-y+1)\Gamma(\alpha)\Gamma(\beta)\Gamma(\alpha+\beta+n)}
\]
\item Let $\tilde{Y}$ the variable that counts the number of positive tests in the second trial and $m=10$. Since $\tilde{Y}\sim\mathrm{Bin}(m,\theta)$ compute the predictive distribution as follows.
\begin{align*}
P(\tilde{Y}=\tilde{y}\mid Y=y)& = \int P(\tilde{Y} = \tilde{y}\mid \theta)P(\theta\mid Y = y)d\theta\\
& = C\int \theta^{\tilde{y}}(1-\theta)^{m-\tilde{y}}\theta^{\alpha+y-1}(1-\theta)^{\beta+n-y-1}d\theta\\
& = C\int \theta^{\alpha+y+\tilde{y}-1}(1-\theta)^{\beta+n+m-y-\tilde{y}-1}d\theta\\
&= C\frac{\Gamma(\alpha+y+\tilde{y})\Gamma(\beta+n+m-y-\tilde{y})}{\Gamma(\alpha+\beta+n+m)}
\end{align*}
where 
$C=\binom{m}{\tilde{y}}\frac{\Gamma(\alpha+\beta+n)}{\Gamma(\alpha+y)\Gamma(\beta+n-y)}$.
\end{enumerate}
\end{sol}

\begin{exercise}{A2}
\end{exercise}
\begin{sol}
\begin{enumerate}[(a)]
We have $P(Y_i=y\mid \theta) = \theta\exp(-\theta y)$
\item We start computing the logarithm 
\[\log P(Y_i=y\mid \theta) = \log(\theta)-\theta y\]
and then the derivatives
\begin{align*}
\parcial{}{\theta} P(Y_i=y\mid \theta)  =  \frac{1}{\theta} - y \\
\secondparcial{}{\theta} P(Y_i=y\mid \theta)  = - \frac{1}{\theta^2}
\end{align*}
By definition of Fisher information we have
\begin{align*}
I(\theta) & = -E_{Y_i\mid\theta}\left(\secondparcial{}{\theta} P(Y_i\mid \theta)\right)\\
& = - E_{Y\mid\theta}(-\frac{1}{\theta^2})\\
& = \frac{1}{\theta^2}
\end{align*}
Jeffrey's prior is then $I(\theta)^{\frac{1}{2}}=\frac{1}{\theta}\sim \mathrm{Gamma}(0,0)$.
\item In general we have
\[P(\phi) = P(\theta)\left|\frac{d\theta}{d\phi}\right|\]
so if we want a uniform prior for $\phi$ we need to find a transformation such that $P(\theta)\left|\frac{d\theta}{d\phi}\right|$ is constant. Since we are using Jeffrey's prior for $\theta$, this is equivalent to $\left|\frac{d\theta}{d\phi}\right|\propto \theta$. One can achieve that by taking $\theta = \exp(\phi)$, or equivalently $\phi = \log(\theta)$, so that $g = \log$. 
\item The posterior is computed as follows
\begin{align*}
P(\theta\mid Y = y)& \propto P(Y=y\mid \theta)P(\theta)\\
& = \theta^n \exp\left(-\theta\sum_{i=1}^n y_i\right)\frac{1}{\theta}\\
& = \theta^{n-1} \exp\left(-\theta n \bar{y}\right)
\end{align*}
This implies that the posterior of $\theta$ follows a $\mathrm{Gamma}(n, n\bar{y})$. 
\item The predictive distribution is computed as follows 
\begin{align*}
P(\tilde{Y}=\tilde{y}\mid Y=y)& = \int P(\tilde{Y} = \tilde{y}\mid \theta)P(\theta\mid Y = y)d\theta\\
& \propto \int \theta\exp(-\theta\tilde{y})\theta^{n-1} \exp\left(-\theta n \bar{y}\right)d\theta\\
& = \int \theta^{n} \exp\left(-\theta (n \bar{y}+\tilde{y})\right)d\theta\\
&= 1
\end{align*}
The last equality follows from the distribution $\mathrm{Gamma}(n+1,n \bar{y}+\tilde{y})$. We deduce that the predictive distribution is uniform.
\end{enumerate}
\end{sol}

\begin{exercise}{B1}
\end{exercise}
\begin{sol}
\begin{enumerate}[(a)]
\item $P(A)P(B)P(C\mid A,B)P(D\mid B) P(E\mid C)P(F\mid C,D,E)$
\item The moral graph is the following

\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1.0cm,y=1.0cm]
\clip(-3.133333333333335,-0.78) rectangle (6.666666666658,3.686666666666666);
\draw (0.,3.)-- (3.,3.);
\draw (3.,3.)-- (6.,3.);
\draw (6.,0.)-- (6.,3.);
\draw (3.,0.)-- (6.,0.);
\draw (3.,0.)-- (3.,3.);
\draw (0.,3.)-- (0.,0.);
\draw (0.,0.)-- (3.,0.);
\draw (3.,0.)-- (6.,0.);
\draw (3.,0.)-- (6.,3.);
\draw (3.,3.)-- (6.,0.);
\draw (0.,0.)-- (3.,3.);
\begin{scriptsize}
\draw [fill=black] (0.,3.) circle (2.5pt);
\draw[color=black] (0.09333333333333028,3.25333333333333) node {$A$};
\draw [fill=black] (0.,0.) circle (2.5pt);
\draw[color=black] (0.09333333333333028,0.25333333333332886) node {$B$};
\draw [fill=black] (3.,3.) circle (2.5pt);
\draw[color=black] (3.0933333333333297,3.25333333333333) node {$C$};
\draw [fill=black] (3.,0.) circle (2.5pt);
\draw[color=black] (3.0933333333333297,0.25333333333332886) node {$D$};
\draw [fill=black] (6.,3.) circle (2.5pt);
\draw[color=black] (6.093333333333329,3.25333333333333) node {$E$};
\draw [fill=black] (6.,0.) circle (2.5pt);
\draw[color=black] (6.093333333333329,0.25333333333332886) node {$F$};
\end{scriptsize}
\end{tikzpicture}
\begin{enumerate}[(i)]
\item True since the the moral graph adds an edge on the subgraph generated by $\{A,B\}$.
\item False because the moral graph doesn't add an edge in the subgraph generated by $\{A,C,E\}$.
\end{enumerate}
\end{enumerate}
\end{sol}

\begin{exercise}{B2}
\end{exercise}
\begin{sol}
\begin{enumerate}[(a)]
\item The DAG looks as follows.

\begin{tikzpicture}[scale=2,line cap=round,line join=round,>=triangle 45,x=1.0cm,y=1.0cm]
\clip(0.02333333333333283,0.1) rectangle (3.854444444444445,3.0333333333333274);
\draw(1.6188888888888886,0.8777777777777761) -- (1.014444444444444,0.8822222222222205) -- (1.01,0.2777777777777763) -- (1.6144444444444441,0.2733333333333318) -- cycle;
\draw(2.6,2.) -- (0.8,2.) -- (0.8,0.2) -- (2.6,0.2) -- cycle;
\draw(2.3877777777777776,0.8733333333333316) -- (1.81,0.8688888888888888) -- (1.8144444444444425,0.2911111111111115) -- (2.39222222222222,0.29555555555555396) -- cycle;
\draw(1.3744444444444446,2.5933333333333293) circle (0.3340529269811789cm);
\draw(2.174444444444444,2.5933333333333293) circle (0.3489048120924053cm);
\draw(1.2277777777777774,1.602222222222219) circle (0.26666666666666655cm);
\draw (1.6188888888888886,0.8777777777777761)-- (1.014444444444444,0.8822222222222205);
\draw (1.014444444444444,0.8822222222222205)-- (1.01,0.2777777777777763);
\draw (1.01,0.2777777777777763)-- (1.6144444444444441,0.2733333333333318);
\draw (1.6144444444444441,0.2733333333333318)-- (1.6188888888888886,0.8777777777777761);
\draw (2.6,2.)-- (0.8,2.);
\draw (0.8,2.)-- (0.8,0.2);
\draw (0.8,0.2)-- (2.6,0.2);
\draw (2.6,0.2)-- (2.6,2.);
\draw [->] (1.348212114330385,2.26031197834017) -- (1.1655555555555552,1.8555555555555518);
\draw [->] (2.0448944571415244,2.269371346842553) -- (1.29,1.8644444444444408);
\draw [->] (1.2470509139320365,1.336252943293448) -- (1.2749419568822549,0.8803067993366483);
\draw (1.25,2.7) node[anchor=north west] {$\alpha$};
\draw (2.081111111111111,2.753333333333328) node[anchor=north west] {$\beta$};
\draw (1.094444444444444,1.7666666666666633) node[anchor=north west] {$\theta_1$};
\draw(2.2,1.6) circle (0.25706078447242453cm);
\draw (2.3877777777777776,0.8733333333333316)-- (1.81,0.8688888888888888);
\draw (1.81,0.8688888888888888)-- (1.8144444444444425,0.2911111111111115);
\draw (1.8144444444444425,0.2911111111111115)-- (2.39222222222222,0.29555555555555396);
\draw (2.39222222222222,0.29555555555555396)-- (2.3877777777777776,0.8733333333333316);
\draw [->] (2.183270705901152,1.3434841571510008) -- (2.1166666666666667,0.8866666666666649);
\draw [->] (1.5309491192558253,2.2982102322604434) -- (2.018230576122415,1.781769423877585);
\draw [->] (2.206032801905055,2.245861401266612) -- (2.227777777777778,1.8555555555555518);
\draw (2.09,1.775555555555552) node[anchor=north west] {$\theta_2$};
\draw (1.1566666666666663,0.7222222222222208) node[anchor=north west] {$Y_1$};
\draw (2.0233333333333334,0.7044444444444431) node[anchor=north west] {$Y_2$};
\end{tikzpicture}
\item The full conditional distribution of $\theta_1$ is
\begin{align*}
p(\theta_1\mid Y_1,Y_2,\theta_2,\alpha,\beta)&=p(\theta_1\mid \alpha,\beta)p(Y_1\mid \theta_1)\\
&\propto \theta_1^{\alpha-1}(1-\theta_1)^{\beta-1}\theta^r(1-\theta)^{y_1}\\
&=\theta_1^{\alpha+r-1}(1-\theta_1)^{\beta+y_1-1}.
\end{align*}
So the full conditional distribution of $\theta_1$ is a $\mathrm{Beta}(\alpha+r,\beta+y_1)$.
\item The posterior of $\theta_1$ coincides with the full-conditional distribution 
\[
P(\theta_1\mid \alpha,\beta,\theta_2,Y_1,Y_2)=P(\theta_1\mid\alpha,\beta)P(Y_1\mid\theta_1)
\]
So we can use the previously sampled values in the Gibss sampler to obtain the new distribution for $\theta_1$, and the update the Gibss sampler with this new distribution.
%This is not in the lecture notes. An explanation in general can be found in \href{https://en.wikipedia.org/wiki/Gibbs_sampling#Implementation}{Wikipedia}.
\end{enumerate}

\end{sol}


\begin{exercise}{B3}
\end{exercise}
\begin{sol}
\begin{enumerate}[(a)]
\item 
\begin{align*}
E_{\theta\mid y}\left[(t-\theta)^2\right] & = t^2 + E_{\theta\mid y}(\theta^2)-2tE_{\theta\mid y}(\theta)\\
& =  E_{\theta\mid y}(\theta^2) -  E_{\theta\mid y}(\theta)^2+ \left(E_{\theta\mid y}(\theta)-t\right)^2\\
& =  E_{\theta\mid y}\left[ (E_{\theta\mid y}(\theta)-\theta)^2\right]+\left(E_{\theta\mid y}(\theta)-t\right)^2\\
& = \mathrm{var}_{\theta\mid y}(\theta)+\left(E_{\theta\mid y}(\theta)-t\right)^2
\end{align*}
\item For $L(\theta,T(y))=(\theta-T(y))^2$ we are precisely in the situation of the previous part. Therefore, the expected value is
\[
E_{\theta\mid y}\left[(T(y)-\theta)^2\right] = \mathrm{var}_{\theta\mid y}(\theta)+\left(E_{\theta\mid y}(\theta)-T(y)\right)^2
\]
As a function of $T(y)$, the expected value clearly attains its minimum at $T(y) = E_{\theta\mid y}(\theta)$, so this should be the suggested estimator.
\end{enumerate}
\end{sol}

\begin{exercise}{B4}
\end{exercise}
\begin{sol}
\begin{enumerate}[(a)]
\item
\begin{enumerate}[(i)]
\item The model $M2$ has one more level in the hierarchy than $M1$. The model $M1$ uses a normal distribution with a huge variance as a non-informative prior, while in the model $M2$ the parameters of this normal distirbution follow themselves non-informative priors. I think the model $M2$ is more realistic because we probably don't know the actual value of the parameters of the normal distribution.
\item The values of the average in set $A$ are closer to each other. This is a sign of a more hierarchical model, so it must correspond to model $M2$. Therefore set $B$ must be from $M1$.
\end{enumerate}
\end{enumerate}
\end{sol}
\end{document}
\documentclass{article}
\usepackage{amsmath,accents}%
\usepackage{amsfonts}%
\usepackage{amssymb}%
\usepackage{comment}
\usepackage{graphicx}
\usepackage{mathrsfs}
\usepackage[utf8]{inputenc}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{mathrsfs}
\usepackage{setspace}  
\usepackage{amsthm}
\usepackage{nccmath}
\usepackage[UKenglish]{babel}
\usepackage{multirow}
\usepackage{enumerate}
\usepackage{listings}

\theoremstyle{plain}

\renewcommand{\baselinestretch}{1,4}
\setlength{\oddsidemargin}{0.5in}
\setlength{\evensidemargin}{0.5in}
\setlength{\textwidth}{5.4in}
\setlength{\topmargin}{-0.25in}
\setlength{\headheight}{0.5in}
\setlength{\headsep}{0.6in}
\setlength{\textheight}{8in}
\setlength{\footskip}{0.75in}

\newtheorem{theorem}{Teorema}[section]
\newtheorem{acknowledgement}{Acknowledgement}
\newtheorem{algorithm}{Algorithm}
\newtheorem{axiom}{Axiom}
\newtheorem{case}{Case}
\newtheorem{claim}{Claim}
\newtheorem{propi}[theorem]{Propiedades}
\newtheorem{condition}{Condition}
\newtheorem{conjecture}{Conjecture}
\newtheorem{coro}[theorem]{Corolario}
\newtheorem{criterion}{Criterion}
\newtheorem{defi}[theorem]{Definición}
\newtheorem{example}[theorem]{Ejemplo}

\theoremstyle{definition}
\newtheorem{exercise}{Exercise}
\newtheorem{lemma}[theorem]{Lema}
\newtheorem{nota}[theorem]{Nota}
\newtheorem{sol}{Solución}
\newtheorem*{sol*}{Solution}
\newtheorem{prop}[theorem]{Proposición}
\newtheorem{remark}{Remark}

\newtheorem{dem}[theorem]{Demostración}

\newtheorem{summary}{Summary}

\providecommand{\abs}[1]{\lvert#1\rvert}
\providecommand{\norm}[1]{\lVert#1\rVert}
\providecommand{\ninf}[1]{\norm{#1}_\infty}
\providecommand{\numn}[1]{\norm{#1}_1}
\providecommand{\gabs}[1]{\left|{#1}\right|}
\newcommand{\bor}[1]{\mathcal{B}(#1)}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\X}{\chi}
\providecommand{\Zn}[1]{\Z / \Z #1}
\newcommand{\resi}{\varepsilon_L}
\newcommand{\cee}{\mathbb{C}}
\providecommand{\conv}[1]{\overset{#1}{\longrightarrow}}
\providecommand{\gene}[1]{\langle{#1}\rangle}
\providecommand{\convcs}{\xrightarrow{CS}}
% xrightarrow{d}[d]
\setcounter{exercise}{0}
\newcommand{\cicl}{\mathcal{C}}

\newenvironment{ejercicio}[2][Estado]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries Ejercicio}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
%--------------------------------------------------------
\begin{document}

\title{Applied Bayesian Methods - Week 1 Exercises }
\author{Javier Aguilar Martín}
\date{\today}
\maketitle
\begin{exercise}
A test to detect drunk drivers has probability of 0.8 of being correct
(i.e. of providing a positive result when the level of alcohol in the
driver's blood is truly over the acceptable limit, and of providing a
negative results when the driver's blood alcohol level is truly below
this limit). If this test is positive, a different test is then carried out.
The second test always correctly detects if the driver is in fact \emph{not}
drunk, but has a 10\% error rate with drunk drivers.
If 20\% of drivers stopped by the police are actually drunk, calculate:

\begin{enumerate}[(a)]
\item the proportion of drivers stopped that have to be given the second
test (i.e. the proportion of drivers testing positive on the first test);
\item the probability that a driver testing positive on the first test really
is drunk;
\item the probability that a driver testing negative on the second test
really is drunk.
\end{enumerate}

\end{exercise}
\begin{sol*}
Let $T_1$ the outcome of the first test (1 if positive, 0 otherwise) and $D$ the event of being drunk (1 if true, 0 otherwise). Similarly, $T_2$ is the outcome of the second test. By abuse of notation let $P(T_1)$ be the probability of $T_1$ being correct. With this notation we have the following probability data:

\begin{itemize}
\item $P(T_1 = 1\mid D = 1)=P(T_1 = 0\mid D = 0)=0.8$.%$P(T_1) = P(T_1 = 1\mid D = 1)+P(T_1 = 0\mid D = 0)=0.8$.

\item $P(T_2 = 0\mid D = 0)= 1$.

\item $P(T_2 = 0\mid D = 1)=0.1$.

\item $P(D=1)=0.2$.
\end{itemize}
We will use the law of total probability and Bayes' theorem to compute the desired probabilities.
\begin{enumerate}[(a)]
\item We need to compute
\begin{align*}
P(T_1=1)&=P(T_1=1\mid D=1)P(D=1)+P(T_1=1\mid D=0)P(D=0)\\
&=0.8\times 0.2 +  0.2\times 0.8 \\
&= 0.32
%&=P(T_1=1\mid D=1)\times 0.2+(1-P(T_1 = 0\mid D = 0))\times 0.8
\end{align*}
%%\begin{align*}
%%P(T_1=1)&=P(T_1=1\mid T_1)P(T_1)+P(T_1=1\mid \neg T_1)P(\neg T_1)\\
%%\end{align*}
%%
%%\begin{align*}
%%P(T_1=1\mid T_1) = \frac{P(T_1\mid T_1=1)P(T_1=1)}{P(T_1)}
%%\end{align*}
%%
%%$P(T_1=1) = P(D=1\cap T_1) + P(D=0\cap \neg T_1)$
%%
%%$P(D=1\cap T_1) = P(T_1\mid  D=1)P(D=1)+ (1-P(T_1\mid D=0))P(D=0)$

\item We need to compute
\[P(D = 1 \mid T_1=1)= \frac{P(T_1=1\mid D=1)P(D=1)}{P(T_1 = 1)}=\frac{0.8\times 0.2}{0.32}=0.5.\]
\item We have to compute
\begin{align*}
P(D=1\mid T_2=0) &= \frac{P(T_2=0\mid D=1)P(D=1)}{P(T_2=0)}\\
&=\frac{P(T_2=0\mid D=1)P(D=1)}{P(T_2=0\mid D=1)P(D=1)+P(T_2=0\mid D=0)P(D=0)}\\
&=\frac{0.1\times 0.2}{0.1\times 0.2 + 1\times 0.8}\\
&=0.024
\end{align*}
\end{enumerate}
\end{sol*}


\newpage
\begin{exercise}
Suppose you are investigating the long-term effects of exposure to low
levels of lead in childhood on reading ability, and your prior distribution
for the proportion of all such children having a reading ability below
expected can be represented by a $\mathrm{Beta}(1, 1)$ distribution (i.e. a uniform
prior distribution).

Now you find a report saying that, in a study, some researchers have
analysed children’s shed primary teeth for lead content, and they found
that 29 children in the study had high lead content and, of these, 7 had
a reading ability of well below that expected for their age.

Based on all the information, of 10 more children found to have a high
lead content in their teeth, what is your (predictive) probability that $\tilde{y}$ children will have lower than expected reading ability for their age?

\emph{Hint: You will need to first evaluate the posterior distribution for the
proportion of children with low reading ability, then obtain the predictive
distribution. It may be useful to note that the normalising constant for
a $\mathrm{Beta}(\alpha, \beta)$ distribution (i.e. $\int^1_{\theta=0} \theta^{\alpha - 1}(1 - \theta)^{\beta - 1}d\theta$) is $\frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}$.}
\end{exercise}
\begin{sol*}
As the hint says, we first need to evaluate the posterior distribution for the proportion of children  with low  reading ability. Let $y=7$ the number of children with low reading ability and $n=29$ the number of trials. We may define a random variable $Y\sim \mathrm{Bin}(n,\theta)$, where $\theta\in\mathrm{Beta}(1,1)$. We have the likelihood 
\begin{align*}
P(Y=y\mid \theta) &= \binom{n}{y}\theta^y (1-\theta)^{n-y}\\
&= \binom{29}{7}\theta^7(1-\theta)^{22}
\end{align*}

Now  we compute the posterior using Bayes' theorem.
\begin{align*}
P(\theta\mid Y=y)&=\frac{P(Y=y\mid\theta)P(\theta)}{P(Y=y)}\\
&=\frac{\binom{29}{7}}{P(Y=y)}\theta^7(1-\theta)^{22}\times 1
\end{align*}


This means that $\theta\mid y\sim \mathrm{Beta}(8,23)$.

We can deduce the value of $P(Y=y)$ from the fact that $\int_{\theta=0}^1P(\theta\mid Y=y)d\theta = 1$. We know that 
\[
\int_{\theta=0}^1\theta^7(1-\theta)^{22}d\theta = \frac{\Gamma(8)\Gamma(23)}{\Gamma(8 + 23)}.
\]

Recalling that $\Gamma(n)=(n-1)!$, this implies that
\[P(Y=y) =  \frac{\Gamma(8)\Gamma(23)}{\Gamma(31)}\frac{\Gamma(30)}{\Gamma(8)\Gamma(23)}=\frac{\Gamma(30)}{\Gamma(31)}=\frac{1}{30}.\]

With this information we can obtain a predictive distribution $\tilde{Y}$ using the formula from the notes. Notice that $\tilde{Y}\sim\mathrm{Bin}(10,\theta)$ and recall that $\theta\mid y\sim\mathrm{Beta}(8,23)$.

\begin{align*}
P(\tilde{Y}=\tilde{y}\mid Y = y) &= \int P(\tilde{Y}=\tilde{y}\mid \theta)P(\theta\mid y)d\theta\\
&=C\int \theta^{\tilde{y}}(1-\theta)^{10-\tilde{y}}\theta^7(1-\theta)^{22}d\theta\\
&=C\int \theta^{\tilde{y}+7}(1-\theta)^{32-\tilde{y}}d\theta\\
&= C \frac{\Gamma(\tilde{y}+8)\Gamma(33-\tilde{y})}{\Gamma(41)},
\end{align*}
where $C=30\times \binom{29}{7}\times \binom{10}{\tilde{y}}$.

\textbf{Extra:} Since there are only ten possible values of $\tilde{Y}$ I have written the following python code that checks that the values obtained define a probability distribution and computes the expected number of children with lower reading abilities.


\lstset{language=Python,
showstringspaces=false,
tab=\rightarrowfill}
\begin{lstlisting}
from scipy.special import gamma, binom
from math import isclose

c = 30*binom(29, 7)

def prob(y):
  binomial = binom(10,y)
  gammas = gamma(y+8)*gamma(33-y)/gamma(41)
  p = c*binomial*gammas
  return  p 

values = range(11)

def expected(values):
  e = sum(y*prob(y) for y in values)
  return e

probs = [prob(y) for y in values]
if all(0<=p<=1 for p in probs):
  print("All values are valid probabilities.")
else:
  raise ValueError("Some values are not valid probabilities.")

if isclose(sum(probs),1):
  print("The probabilities add up to 1.")
else:
  raise ValueError("The probabilities don't add up to 1.")

print("Expected value:",  expected(values))
\end{lstlisting}

As one can check, the output is
\begin{verbatim}
>>> All values are valid probabilities.
The probabilities add up to 1.
Expected value: 2.580645161290322
\end{verbatim}
So our calculations must be correct and the expected number of children with lower reading abilities in the latest trial is between 2 and 3. 
\end{sol*}



\end{document}